{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35b362b",
   "metadata": {},
   "source": [
    "### SMOTEENN Resampling:\n",
    "\n",
    "Reason: Addressing class imbalance (highly imbalanced classes in target variable) by oversampling minority classes and undersampling majority classes simultaneously.\n",
    "Benefit: Helps in improving model performance by making the classes more balanced, thereby reducing bias towards majority classes and improving predictive accuracy.\n",
    "\n",
    "### RobustScaler:\n",
    "\n",
    "Reason: Scaling features using RobustScaler because it is less prone to outliers compared to standard scaling methods like StandardScaler.\n",
    "Benefit: Ensures that features are on the same scale, which is crucial for models like Balanced Random Forest and XGBoost that rely on distance metrics or gradient-based optimization.\n",
    "\n",
    "### Model Selection:\n",
    "Balanced Random Forest Classifier:\n",
    "\n",
    "Reason: Chosen due to its ability to handle imbalanced datasets naturally through class weighting and sampling techniques.\n",
    "Benefit: Provides a balanced approach to classification tasks, maintaining robustness against imbalanced classes without requiring extensive data preprocessing.\n",
    "XGBoost Classifier:\n",
    "\n",
    "Reason: A powerful gradient boosting algorithm known for its high performance on structured datasets and ability to capture complex interactions in data.\n",
    "Benefit: Effective in improving predictive accuracy and handling large datasets with high dimensionality, often outperforming traditional ensemble methods.\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "RandomizedSearchCV and GridSearchCV:\n",
    "Reason: Used to optimize model performance by searching through a specified parameter space and selecting the best hyperparameters.\n",
    "Benefit: Ensures that the models are fine-tuned to achieve optimal performance on the validation set, improving generalization and reducing overfitting.\n",
    "\n",
    "### Evaluation Metrics:\n",
    "Confusion Matrix, Classification Report, ROC AUC Score:\n",
    "Reason: Metrics chosen to comprehensively evaluate model performance across multiple aspects such as precision, recall, F1-score, and ROC AUC.\n",
    "Benefit: Provides a balanced view of how well the models classify each class, detect true positives and negatives, and handle class imbalance and uncertainty.\n",
    "\n",
    "### Conclusion:\n",
    "#### Objective: The goal was to develop robust classifiers capable of accurately predicting the target variable in a dataset with significant class imbalance.\n",
    "#### Approach: By combining SMOTEENN resampling for handling imbalance, robust feature scaling, and leveraging ensemble models like Balanced Random Forest and XGBoost, we aimed to maximize predictive accuracy and generalization.\n",
    "#### Outcome: The models were evaluated using comprehensive metrics to assess their performance, leading to the selection of the best-performing model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9e73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf527b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc41fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import joblib\n",
    "\n",
    "# Data Manipulation and Model Evaluation\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Model Algorithms\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50eec4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smoteenn(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Applies SMOTEENN resampling technique to balance the training data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (DataFrame or ndarray): Input features for training.\n",
    "    - y_train (Series or ndarray): Target labels for training.\n",
    "\n",
    "    Returns:\n",
    "    - X_resampled (DataFrame or ndarray): Resampled features.\n",
    "    - y_resampled (Series or ndarray): Resampled target labels.\n",
    "    \"\"\"\n",
    "    smote_enn = SMOTEENN(random_state=42)\n",
    "    X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3881bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_balanced_random_forest(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains a Balanced Random Forest classifier on the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (DataFrame or ndarray): Training features.\n",
    "    - y_train (Series or ndarray): Training target labels.\n",
    "    - X_test (DataFrame or ndarray): Test features.\n",
    "    - y_test (Series or ndarray): Test target labels.\n",
    "\n",
    "    Returns:\n",
    "    - brf (BalancedRandomForestClassifier): Trained classifier.\n",
    "    - y_pred (Series or ndarray): Predictions on test data.\n",
    "    \"\"\"\n",
    "    # Apply SMOTEENN resampling\n",
    "    X_resampled, y_resampled = apply_smoteenn(X_train, y_train)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train the model\n",
    "    brf = BalancedRandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    brf.fit(X_train_scaled, y_resampled)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = brf.predict(X_test_scaled)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(\"Balanced Random Forest Classifier:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nROC AUC Score:\")\n",
    "    print(roc_auc_score(y_test, brf.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "    \n",
    "    return (brf,y_pred,scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67966572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost classifier on the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (DataFrame or ndarray): Training features.\n",
    "    - y_train (Series or ndarray): Training target labels.\n",
    "    - X_test (DataFrame or ndarray): Test features.\n",
    "    - y_test (Series or ndarray): Test target labels.\n",
    "\n",
    "    Returns:\n",
    "    - xgb_model (XGBClassifier): Trained classifier.\n",
    "    - y_pred (Series or ndarray): Predictions on test data.\n",
    "    \"\"\"\n",
    "    # Apply SMOTEENN resampling\n",
    "    X_resampled, y_resampled = apply_smoteenn(X_train, y_train)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "    xgb_model.fit(X_train_scaled, y_resampled)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = xgb_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(\"\\nXGBoost Classifier:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nROC AUC Score:\")\n",
    "    print(roc_auc_score(y_test, xgb_model.predict_proba(X_test_scaled), multi_class='ovr'))\n",
    "    \n",
    "    return xgb_model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5bea379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(X_train, y_train,X_test,y_test):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning using RandomizedSearchCV and GridSearchCV on Balanced Random Forest.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (DataFrame or ndarray): Training features.\n",
    "    - y_train (Series or ndarray): Training target labels.\n",
    "\n",
    "    Returns:\n",
    "    - best_brf (BalancedRandomForestClassifier): Best tuned classifier.\n",
    "    \"\"\"\n",
    "    # Define the parameter distribution for RandomizedSearchCV\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(100, 300),\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': randint(2, 10),\n",
    "        'min_samples_leaf': randint(1, 4)\n",
    "    }\n",
    "    \n",
    "    # Apply SMOTEENN resampling\n",
    "    X_resampled, y_resampled = apply_smoteenn(X_train, y_train)\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Create StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    brf = BalancedRandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    \n",
    "    # Initialize RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(estimator=brf, param_distributions=param_dist, n_iter=50, cv=skf, \n",
    "                                       scoring='f1_weighted', n_jobs=-1, random_state=42, verbose=3)\n",
    "    \n",
    "    # Fit RandomizedSearchCV\n",
    "    random_search.fit(X_train_scaled, y_resampled)\n",
    "    \n",
    "    # Best parameters from RandomizedSearchCV\n",
    "    best_params_random = random_search.best_params_\n",
    "    print(\"\\nBest parameters from RandomizedSearchCV: \", best_params_random)\n",
    "    \n",
    "    # Define the refined parameter grid for GridSearchCV\n",
    "    param_grid_fine = {  \n",
    "        'n_estimators': [best_params_random['n_estimators']-50, best_params_random['n_estimators'], best_params_random['n_estimators']+50],  \n",
    "        'max_features': [best_params_random['max_features']],  \n",
    "        'max_depth': [best_params_random['max_depth']-5, best_params_random['max_depth'], best_params_random['max_depth']+5] if best_params_random['max_depth'] else [None],  \n",
    "        'min_samples_split': [best_params_random['min_samples_split']-1, best_params_random['min_samples_split'], best_params_random['min_samples_split']+1],  \n",
    "        'min_samples_leaf': [best_params_random['min_samples_leaf']-1, best_params_random['min_samples_leaf'], best_params_random['min_samples_leaf']+1]  \n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV for refined tuning\n",
    "    grid_search_fine = GridSearchCV(estimator=brf, param_grid=param_grid_fine, cv=skf, \n",
    "                                    scoring='f1_weighted', n_jobs=-1, verbose=3)\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search_fine.fit(X_train_scaled, y_resampled)\n",
    "    \n",
    "    # Best parameters and estimator from GridSearchCV\n",
    "    best_params_fine = grid_search_fine.best_params_\n",
    "    print(\"\\nBest parameters from GridSearchCV: \", best_params_fine)\n",
    "    best_brf = grid_search_fine.best_estimator_\n",
    "    \n",
    "    return best_brf,scaler,X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7691d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_scaler(model, scaler, model_filename='best_model.pkl', scaler_filename='scaler.pkl'):\n",
    "    \"\"\"\n",
    "    Saves the trained model and scaler to disk.\n",
    "\n",
    "    Parameters:\n",
    "    - model (object): Trained model object.\n",
    "    - scaler (object): Scaler object used for preprocessing.\n",
    "    - model_filename (str): Filename for saving the model (default: 'best_model.pkl').\n",
    "    - scaler_filename (str): Filename for saving the scaler (default: 'scaler.pkl').\n",
    "    \"\"\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "    print(f\"Scaler saved as {scaler_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a98e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_scaler(model_filename='best_model.pkl', scaler_filename='scaler.pkl'):\n",
    "    \"\"\"\n",
    "    Loads a trained model and scaler from disk.\n",
    "\n",
    "    Parameters:\n",
    "    - model_filename (str): Filename of the saved model (default: 'best_model.pkl').\n",
    "    - scaler_filename (str): Filename of the saved scaler (default: 'scaler.pkl').\n",
    "\n",
    "    Returns:\n",
    "    - model (object): Loaded model object.\n",
    "    - scaler (object): Loaded scaler object.\n",
    "    \"\"\"\n",
    "    model = joblib.load(model_filename)\n",
    "    scaler = joblib.load(scaler_filename)\n",
    "    print(f\"Loaded model from {model_filename}\")\n",
    "    print(f\"Loaded scaler from {scaler_filename}\")\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497257d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e92b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6aeb0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test_scaled, y_test):\n",
    "    \"\"\"Evaluate models: print confusion matrix, classification report, and ROC AUC score.\"\"\"\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        print(\"\\nROC AUC Score:\")\n",
    "        print(roc_auc_score(y_test, model.predict_proba(X_test_scaled), multi_class='ovr'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150e64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7183c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2b0fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read X_train, y_train, X_test, y_test from saved files\n",
    "    # Reading X_train, X_test\n",
    "    X_train = pd.read_csv('X_train.csv')\n",
    "    X_test = pd.read_csv('X_test.csv')\n",
    "\n",
    "    # Reading y_train, y_test (assuming headers are present)\n",
    "    y_train = pd.read_csv('y_train.csv') # Read as Series\n",
    "    y_test = pd.read_csv('y_test.csv')\n",
    "    # Train models and perform evaluations\n",
    "    brf, y_pred_brf,scaler = train_balanced_random_forest(X_train, y_train, X_test, y_test)\n",
    "    xgb_model, y_pred_xgb = train_xgboost(X_train, y_train, X_test, y_test)\n",
    "    # Hyperparameter tuning\n",
    "    best_brf,scaler,X_test_scaled = tune_model(X_train, y_train,X_test,y_test)\n",
    "    \n",
    "    # Save the best model and scaler\n",
    "    save_model_and_scaler(best_brf, scaler)\n",
    "    \n",
    "    # Optionally, load the model and scaler\n",
    "    loaded_model, loaded_scaler = load_model_and_scaler()\n",
    "    evaluate_model(loaded_model,X_test_scaled,y_test)\n",
    "    print(\"\\nPredictions using loaded model:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54c1da4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Random Forest Classifier:\n",
      "Confusion Matrix:\n",
      "[[4627  219    1  102   51]\n",
      " [  26   20    0    3    2]\n",
      " [   2    0   28    0    0]\n",
      " [  17    7    0    1    1]\n",
      " [   8    5    0    2    0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96      5000\n",
      "           1       0.08      0.39      0.13        51\n",
      "           2       0.97      0.93      0.95        30\n",
      "           3       0.01      0.04      0.01        26\n",
      "           4       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.91      5122\n",
      "   macro avg       0.41      0.46      0.41      5122\n",
      "weighted avg       0.97      0.91      0.94      5122\n",
      "\n",
      "\n",
      "ROC AUC Score:\n",
      "0.8227358345324424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:05:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-06abd128ca6c1688d-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Classifier:\n",
      "Confusion Matrix:\n",
      "[[4501  264    6  150   79]\n",
      " [  21   20    1    6    3]\n",
      " [   1    0   28    0    1]\n",
      " [  14    7    0    3    2]\n",
      " [   7    5    0    2    1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94      5000\n",
      "           1       0.07      0.39      0.12        51\n",
      "           2       0.80      0.93      0.86        30\n",
      "           3       0.02      0.12      0.03        26\n",
      "           4       0.01      0.07      0.02        15\n",
      "\n",
      "    accuracy                           0.89      5122\n",
      "   macro avg       0.38      0.48      0.39      5122\n",
      "weighted avg       0.97      0.89      0.93      5122\n",
      "\n",
      "\n",
      "ROC AUC Score:\n",
      "0.8193223876786785\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "81 fits failed out of a total of 150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "23 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\base.py\", line 42, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\utils\\_param_validation.py\", line 105, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "imblearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of BalancedRandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "58 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\base.py\", line 42, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\utils\\_param_validation.py\", line 105, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "imblearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of BalancedRandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.99696517        nan        nan\n",
      " 0.99605863 0.99619428        nan        nan        nan        nan\n",
      " 0.99696505 0.99705638 0.99673844 0.9969654  0.99669304 0.99660245\n",
      "        nan 0.98177444        nan        nan        nan 0.99619443\n",
      " 0.98082781 0.99673927        nan        nan 0.99601307 0.99651201\n",
      " 0.9959677         nan        nan        nan        nan        nan\n",
      " 0.99682911        nan 0.99678435 0.99701067        nan        nan\n",
      " 0.99692    0.9814588  0.99655778        nan 0.99614884        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters from RandomizedSearchCV:  {'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 149}\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "81 fits failed out of a total of 243.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "81 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\base.py\", line 42, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\utils\\_param_validation.py\", line 105, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "imblearn.utils._param_validation.InvalidParameterError: The 'min_samples_leaf' parameter of BalancedRandomForestClassifier must be an int in the range [1, inf) or a float in the range (0.0, 1.0). Got 0 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.99705637 0.99723728 0.99710111\n",
      " 0.99687484 0.99705638 0.99710155 0.99741874 0.99737361 0.99728267\n",
      " 0.9968301  0.99664824 0.99669341 0.9967844  0.99678424 0.99660252\n",
      " 0.9968752  0.99673899 0.99682996        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.99705637 0.99723728 0.99710111 0.99687484 0.99705638 0.99710155\n",
      " 0.99741874 0.99737361 0.99728267 0.9968301  0.99669364 0.99669341\n",
      " 0.99673903 0.99682943 0.99660252 0.9968752  0.99673899 0.99682996\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.99705637 0.99723728 0.99710111\n",
      " 0.99687484 0.99705638 0.99710155 0.99741874 0.99737361 0.99728267\n",
      " 0.9968301  0.99669364 0.99669341 0.99673903 0.99682943 0.99660252\n",
      " 0.9968752  0.99673899 0.99682996]\n",
      "  warnings.warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\imblearn\\ensemble\\_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "c:\\Users\\E009819\\AppData\\Local\\miniconda3\\lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters from GridSearchCV:  {'max_depth': 25, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 6, 'n_estimators': 99}\n",
      "Model saved as best_model.pkl\n",
      "Scaler saved as scaler.pkl\n",
      "Loaded model from best_model.pkl\n",
      "Loaded scaler from scaler.pkl\n",
      "Confusion Matrix:\n",
      "[[4618  225    1  101   55]\n",
      " [  29   18    0    2    2]\n",
      " [   3    0   27    0    0]\n",
      " [  17    6    0    2    1]\n",
      " [   8    3    0    2    2]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95      5000\n",
      "           1       0.07      0.35      0.12        51\n",
      "           2       0.96      0.90      0.93        30\n",
      "           3       0.02      0.08      0.03        26\n",
      "           4       0.03      0.13      0.05        15\n",
      "\n",
      "    accuracy                           0.91      5122\n",
      "   macro avg       0.42      0.48      0.42      5122\n",
      "weighted avg       0.97      0.91      0.94      5122\n",
      "\n",
      "\n",
      "ROC AUC Score:\n",
      "0.8247766033213548\n",
      "\n",
      "Predictions using loaded model:\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863794c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
